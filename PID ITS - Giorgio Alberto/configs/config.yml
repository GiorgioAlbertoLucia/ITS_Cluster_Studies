
# Configuration file for the particle identification with the ML algorithm in PID_ITS - Giorgio Alberto

input:
    data:       /data/shared/ITS/ML/particles_pid_520143.parquet
    appl_data:  /data/shared/ITS/ML/particles_pid_520147.parquet
    ext_appl: True

    isV0: True



output:
    data_visual_dir:    ../../../data_visual_root
    ml_dir:             ../../../ML_output
    final_dir:          ../../../final_plots

    save_data_dir:      ../../../data
    model_out:          ../../../ML_output

    #particle:   { 0: "Deu", 1: "P", 2: "K", 3: "Pi"}
    particle:   { 1: "P", 2: "K", 3: "Pi"}


data_prep:

    skip_data_prep: False
    prep_data: [/Users/giogi/Desktop/Stage INFN/PID ITS/data/TrainSet_augm.parquet.gzip,
                /Users/giogi/Desktop/Stage INFN/PID ITS/data/yTrain_augm.parquet.gzip,
                /Users/giogi/Desktop/Stage INFN/PID ITS/data/TestSet_augm.parquet.gzip,
                /Users/giogi/Desktop/Stage INFN/PID ITS/data/yTest_betaflataugm.gzip]
    appl_data:  /Users/giogi/Desktop/Stage INFN/PID ITS/data/ApplicationDf_augm.parquet.gzip
    save_data: True

    test_frac: 0.2
    seed_split: 0   # random_state for train_test_split

    # Seven Hits (only consider candidates with hits on all the layeres)
    #___________________________________
    seven_hits: False
 
    # Equal
    #___________________________________
    do_equal: False


    # Data Augmentation
    #____________________________________
    do_augm: False
    to_augm: ["K", "K"]
    mothers: ["Pi", "P"]
    #p_ranges: [[0., 0.2]]
    p_ranges: [[0., 0.7], [0.6, 1.5]]

    # Beta Flat
    #____________________________________
    betamins: [0.2, 0.4, 0.6, 0.8]
    betamaxs: [0.4, 0.6, 0.8, 1.0]

    # Beta p Flat
    #____________________________________
    pmins: [0., 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0, 1.05, 1.1, 1.15, 1.2, 1.25, 1.3, 1.35, 1.4, 1.45]
    pmaxs: [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0, 1.05, 1.1, 1.15, 1.2, 1.25, 1.3, 1.35, 1.4, 1.45, 1.5]

training:

    skip_training: False

    random_state: 0
    
    RegressionColumns: ["p", 
                        "tgL", 
                        "meanSnPhi", "meanPattID", "clSizeCosLam",
                        "ClSizeL0", "ClSizeL1", "ClSizeL2", "ClSizeL3", "ClSizeL4", "ClSizeL5", "ClSizeL6"]
    ModelParams:        {"n_jobs": 40,
                        #"time_budget": 100,
                        #"metric": "r2",
                        #"task": "regression",
                        #"log_file_name": "bdt.log",
                        "tree_method": "hist", # not available in FLAML
                        #"estimator_list" : ["xgboost"], # not available in FLAML
                        "colsample_bylevel": 0.6337942873486531, # not available in FLAML 
                        "colsample_bytree": 0.7, # not available in FLAML
                        "subsample": 0.710841077866278, # not available in FLAML
                        "learning_rate": 0.04952863262192068, # not available in FLAML
                        "n_estimators": 400, # not available in FLAML
                        "max_depth": 13, # not available in FLAML
                        "min_child_weight": 10, # not available in FLAML
                        "eval_metric": "rmse", # not available in FLAML
                        "reg_alpha": 0.349923237394973, # not available in FLAML
                        "reg_lambda": 0.5031161568154017, # not available in FLAML
                        "verbosity": 1,
                        } # dict of hyperparameters (XGB and AUTOML USE DIFFERENT HYPER-PARS!)

    HyperParamsRange:   {"max_depth": [8, 18], 
                        "learning_rate": [0.01, 0.1],
                        "n_estimators": [300, 1500], 
                        "min_child_weight":[1, 10],
                        "subsample": [0.8, 1.], 
                        "colsample_bytree": [0.8, 1],
                        "n_jobs": [35, 40]}                      # dict of ranges for the hyperparameters evaluated by optuna

    model: "xgboost"        # accepted models: xgboost, automl
    do_opt: True
    early_stop: False
    save_model: True

    beta_flat: False
    beta_p_flat: True

application:
    skip_appl: False
    model_loc: /home/galucia/PID_ITS/ML_output/RegressorModel_xgboost_beta_pflat_.pickle

plots:
    do_plots: True
    vars_to_plot:   ["ClSizeL0", "ClSizeL1", "ClSizeL2", "ClSizeL3", "ClSizeL4", "ClSizeL5", "ClSizeL6", "clSizeCosLam",
                    "meanSnPhi", "tgL", "meanPattID", "p"]
    #vars_to_plot:   ["ClSizeL0", "ClSizeL1", "ClSizeL2", "ClSizeL3", "ClSizeL4", "ClSizeL5", "ClSizeL6", "meanClsize",
    #                "meanSnPhi", "tgL", "meanPattID", "p", "clSizeCosLam"]

    plot_spec_hist: {"ClSizeL0": [10, 0, 10], "ClSizeL1": [10, 0, 10], "ClSizeL2": [10, 0, 10], "ClSizeL3": [10, 0, 10], 
                    "ClSizeL4": [10, 0, 10], "ClSizeL5": [10, 0, 10], "ClSizeL6": [10, 0, 10], "clSizeCosLam": [100, 0, 10],
                    "SnPhiL0": [100, -1, 1], "SnPhiL1": [100, -1, 1], "SnPhiL2": [100, -1, 1], "SnPhiL3": [100, -1, 1], 
                    "SnPhiL4": [100, -1, 1], "SnPhiL5": [100, -1, 1], "SnPhiL6": [100, -1, 1], "meanSnPhi": [100, -1, 1], 
                    "TanLamL0": [100, -10, 10], "TanLamL1": [100, -10, 10], "TanLamL2": [100, -10, 10], "TanLamL3": [100, -10, 10], 
                    "TanLamL4": [100, -10, 10], "TanLamL5": [100, -10, 10], "TanLamL6": [100, -10, 10], "tgL": [100, -10, 10],
                    "PattIDL0": [10, 0, 100], "PattIDL1": [10, 0, 100], "PattIDL2": [10, 0, 100], "PattIDL3": [10, 0, 100], 
                    "PattIDL4": [10, 0, 100], "PattIDL5": [10, 0, 100], "PattIDL6": [10, 0, 100], "meanPattID": [10, 0, 100],
                    "L6_L0": [100, 0, 10], "p": [200, 0, 2]}     # dict (variable_to_plot, hist_settings) 
    
    plot_x_scat:    ["p", "p", "p", "beta"]
    plot_y_scat:    ["dedx", "beta", "clSizeCosLam", "clSizeCosLam"]
    plot_spec_scat: [["p", "#frac{dE}{dx}", 1500, 0, 1.5, 600, 0, 600], ["p", "#beta", 1500, 0, 1.5, 1000, 0, 1.1], ["p", "clSizeCosLam", 150, 0, 1.5, 120, 0, 12], ["#beta", "clSizeCosLam", 100, 0, 1, 120, 0, 12]] 
    bp_flat_scat:   {"p": ["p", "Particle species", 15, 0, 1.5, 4, 0., 4.5], "weights": ["weights", "Particle species", 100, 0, 1, 4, 0., 4.5]}

    model_train:    {"p": [1500, 0, 1.5, 3000, -1.5, 1.5], "beta": [1000, 0, 1, 3000, -1.5, 1.5]}
    appl_plot_spec: {"b_vs_p_final": ["p", "#beta", 1500, 0, 1.5, 1050, 0, 1.05], "b_vs_p_true": ["p", "#beta", 1500, 0, 1.5, 1050, 0, 1.05], 
                    "dedx_vs_p": ["p", "#frac{dE}{dx}", 1500, 0, 1.5, 600, 0, 600], "beta": [1000, 0, 1, 3000, -1.5, 1.5], "p": [1500, 0, 1.5, 3000, -1.5, 1.5]} 